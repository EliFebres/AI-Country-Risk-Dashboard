import re
import html
import httpx
import asyncio
import feedparser
import trafilatura
import datetime as dt
import logging

from typing import List, Dict
from urllib.parse import urlencode, quote_plus, urlparse

from backend.utils.news_fetching.url_resolver import resolve_google_news_url


# Quiet noisy warnings from trafilatura
logging.getLogger("trafilatura").setLevel(logging.ERROR)
logging.getLogger("trafilatura.core").setLevel(logging.ERROR)

UA = "Mozilla/5.0 (compatible; ai-country-risk/1.0)"


def _gnews_url(query: str, lang: str = "en", country: str = "US") -> str:
    """Build a properly encoded Google News RSS search URL."""
    base = "https://news.google.com/rss/search"
    hl = f"{lang}-{country}"
    ceid = f"{country}:{lang}"
    params = {"q": query, "hl": hl, "gl": country, "ceid": ceid}
    return f"{base}?{urlencode(params, quote_via=quote_plus)}"


def _strip_html(s: str) -> str:
    """Remove all HTML (including <a> links) and unescape entities."""
    if not s:
        return ""
    s = re.sub(r"<a[^>]*>.*?</a>", "", s, flags=re.S | re.I)  # drop anchors
    s = re.sub(r"<[^>]+>", "", s)                              # drop remaining tags
    s = html.unescape(s)                                       # unescape entities
    s = re.sub(r"\s+", " ", s).strip()                         # collapse whitespace
    return s


def _clip_words(s: str, max_words: int) -> str:
    """Return the first max_words of s (by whitespace)."""
    if not s or max_words <= 0:
        return ""
    parts = s.split()
    if len(parts) <= max_words:
        return s.strip()
    return " ".join(parts[:max_words]).strip()


async def _fetch_text_async(url: str, client: httpx.AsyncClient, max_chars: int = 3000) -> str:
    try:
        # If somehow still a Google News link, resolve it here too
        if "news.google.com" in urlparse(url).netloc:
            try:
                url = resolve_google_news_url(url)
            except Exception:
                pass

        r = await client.get(url, timeout=15)
        r.raise_for_status()
        # Provide URL context to trafilatura for better extraction heuristics
        text = trafilatura.extract(r.text, url=str(r.url)) or ""
        return text[:max_chars]
    except Exception:
        return ""


def _fetch_text_sync(url: str, client: httpx.Client, max_chars: int = 3000) -> str:
    try:
        if "news.google.com" in urlparse(url).netloc:
            try:
                url = resolve_google_news_url(url)
            except Exception:
                pass

        r = client.get(url, timeout=15)
        r.raise_for_status()
        text = trafilatura.extract(r.text, url=str(r.url)) or ""
        return text[:max_chars]
    except Exception:
        return ""


async def _expand_items_async(entries: List[Dict], max_articles: int, max_chars: int) -> List[Dict]:
    urls = [
        (e.get("publisher_link") or e.get("link"))
        for e in entries[:max_articles]
        if (e.get("publisher_link") or e.get("link"))
    ]
    async with httpx.AsyncClient(follow_redirects=True, headers={"User-Agent": UA}) as client:
        texts = await asyncio.gather(
            *(_fetch_text_async(u, client, max_chars) for u in urls),
            return_exceptions=True
        )
    out = []
    for e, t in zip(entries[:max_articles], texts):
        text = "" if isinstance(t, Exception) else (t or "")
        e2 = dict(e)
        e2["text"] = text
        e2["word_count"] = len(text.split())
        out.append(e2)
    return out + entries[max_articles:]


def _expand_items_sync(entries: List[Dict], max_articles: int, max_chars: int) -> List[Dict]:
    urls = [
        (e.get("publisher_link") or e.get("link"))
        for e in entries[:max_articles]
        if (e.get("publisher_link") or e.get("link"))
    ]
    with httpx.Client(follow_redirects=True, headers={"User-Agent": UA}) as client:
        texts = [_fetch_text_sync(u, client, max_chars) for u in urls]
    out = []
    for e, text in zip(entries[:max_articles], texts):
        e2 = dict(e)
        e2["text"] = text or ""
        e2["word_count"] = len((text or "").split())
        out.append(e2)
    return out + entries[max_articles:]


def gnews_rss(
    query: str,
    *,
    max_results: int = 10,
    expand: bool = True,
    extract_chars: int = 3000,
    lang: str = "en",
    country: str = "US",
    build_summary: bool = True,
    summary_words: int = 240,
    max_age_days: int | None = 30,   # limit by age (None = no filter)
) -> List[Dict]:
    """
    Return Google News RSS items. If expand=True, also fetch and extract each article's main text.

    Each item contains:
      - 'title':          str
      - 'link':           str (original Google News link)
      - 'publisher_link': str (resolved publisher URL)
      - 'published':      ISO8601 str or None
      - 'source':         str (publisher name if available)
      - 'snippet':        str (PLAIN TEXT, links removed)
      - 'snippet_html':   str (original RSS summary with HTML)
      - ['text','word_count'] present when expand=True and extraction succeeds
      - ['summary','summary_word_count'] present when build_summary=True

    Args:
        max_age_days: If set, discard items older than this many days (items
                      without a publish date are discarded).
    """
    url = _gnews_url(query, lang=lang, country=country)
    feed = feedparser.parse(url)

    cutoff = None
    if max_age_days is not None:
        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=max_age_days)

    items: List[Dict] = []
    for e in feed.entries:
        # Parse published time (UTC-aware)
        published_dt = None
        if getattr(e, "published_parsed", None):
            published_dt = dt.datetime(*e.published_parsed[:6], tzinfo=dt.timezone.utc)

        # Age filter
        if cutoff is not None:
            if (published_dt is None) or (published_dt < cutoff):
                continue

        raw_summary = getattr(e, "summary", "") or ""
        plain_summary = _strip_html(raw_summary)

        source_title = ""
        src = getattr(e, "source", None)
        if src and hasattr(src, "title"):
            source_title = getattr(src, "title", "") or ""
        elif isinstance(src, str):
            source_title = src

        raw_link = getattr(e, "link", "") or ""
        try:
            publisher_link = resolve_google_news_url(raw_link)
        except Exception:
            publisher_link = raw_link

        items.append({
            "title": getattr(e, "title", "") or "",
            "link": raw_link,                     # keep original for reference
            "publisher_link": publisher_link,     # use this for fetching content
            "published": published_dt.isoformat().replace("+00:00", "Z") if published_dt else None,
            "source": source_title,
            "snippet": plain_summary,
            "snippet_html": raw_summary,
        })

        # Stop once we have enough recent items
        if len(items) >= max_results:
            break

    # Optionally expand with article body text (limit to number of kept items)
    if expand and items:
        try:
            _ = asyncio.get_running_loop()  # raises RuntimeError if none
            # If we're already in an event loop, use sync fallback to avoid nested loop issues
            items = _expand_items_sync(items, max_articles=len(items), max_chars=extract_chars)
        except RuntimeError:
            items = asyncio.run(_expand_items_async(items, max_articles=len(items), max_chars=extract_chars))

    # Build longer plain-text summaries
    if build_summary and items:
        for e in items:
            base = e.get("text") or e.get("snippet") or ""
            summary = _clip_words(base, summary_words)
            e["summary"] = summary
            e["summary_word_count"] = len(summary.split())

    return items
